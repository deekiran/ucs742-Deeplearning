{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e22c5349e1679a2c42633090e8fc17172f46e9ed"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport nltk, re, time\nfrom nltk.corpus import stopwords\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom collections import namedtuple\n\n\ntrain = pd.read_csv(\"../input/labeledTrainData.tsv\", delimiter=\"\\t\")\ntest = pd.read_csv( \"../input/testData.tsv\", delimiter=\"\\t\")\n\ntrain.head(5)\n\ndef clean_text(text, remove_stopwords=True):\n    '''Clean the text, with the option to remove stopwords'''\n    \n    # Convert words to lower case and split them\n    text = text.lower().split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"<br />\", \" \", text)\n    text = re.sub(r\"[^a-z]\", \" \", text)\n    text = re.sub(r\"   \", \" \", text) # Remove any extra spaces\n    text = re.sub(r\"  \", \" \", text)\n    \n    # Return a list of words\n    return(text)\n\ntrain_clean = train\ntest_clean = test\n\ntrain_clean['review'] = train_clean['review'].apply(lambda x : clean_text(x))\ntest_clean['review'] = test_clean['review'].apply(lambda x : clean_text(x))\n\ntest_clean.head(5)\n\n# Tokenize the reviews\nall_reviews = list(train_clean['review']) + list(test_clean['review'])\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(all_reviews)\nprint(\"Fitting is complete.\")\n\ntrain_seq = tokenizer.texts_to_sequences(list(train_clean['review']))\nprint(\"train_seq is complete.\")\n\ntest_seq = tokenizer.texts_to_sequences(list(test_clean['review']))\nprint(\"test_seq is complete\")\n\nmax_review_length = 200\n\ntrain_pad = pad_sequences(train_seq, maxlen = max_review_length)\nprint(\"train_pad is complete.\")\n\ntest_pad = pad_sequences(test_seq, maxlen = max_review_length)\nprint(\"test_pad is complete.\")\n\nx_train, x_valid, y_train, y_valid = train_test_split(train_pad, train.sentiment, test_size = 0.15, random_state = 2)\n\n\ndef get_batches(x, y, batch_size):\n    '''Create the batches for the training and validation data'''\n    n_batches = len(x)//batch_size\n    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n        \ndef get_test_batches(x, batch_size):\n    '''Create the batches for the testing data'''\n    n_batches = len(x)//batch_size\n    x = x[:n_batches*batch_size]\n    for ii in range(0, len(x), batch_size):\n        yield x[ii:ii+batch_size]\n\ndef build_rnn(n_words, embed_size, batch_size, lstm_size, num_layers, dropout, learning_rate, multiple_fc, fc_units):\n    '''Build the Recurrent Neural Network'''\n\n    tf.reset_default_graph()\n\n    # Declare placeholders we'll feed into the graph\n    with tf.name_scope('inputs'):\n        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n        print(\"Input = \", inputs.shape)\n\n    with tf.name_scope('labels'):\n        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n        print(\"labels = \", labels.shape)\n\n    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n\n    # Create the embeddings\n    with tf.name_scope(\"embeddings\"):\n        embedding = tf.Variable(tf.random_uniform((n_words,embed_size), -1, 1))\n        print(\"embeddings = \", embedding.shape)\n        embed = tf.nn.embedding_lookup(embedding, inputs)\n        print(\"embed = \", embed.shape)\n\n    # Build the RNN layers\n    with tf.name_scope(\"RNN_layers\"):\n        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n        #print(lstm.output_size)\n        drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n        #print(drop.output_shape)\n        #cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n        cell = drop\n        #print(cell.shape)\n    \n    # Set the initial state\n    with tf.name_scope(\"RNN_init_state\"):\n        initial_state = cell.zero_state(batch_size, tf.float32)\n\n    # Run the data through the RNN layers\n    with tf.name_scope(\"RNN_forward\"):\n        outputs, final_state = tf.nn.dynamic_rnn(cell, embed,initial_state=initial_state)    \n    \n    # Create the fully connected layers\n    with tf.name_scope(\"fully_connected\"):\n        \n        # Initialize the weights and biases\n        weights = tf.truncated_normal_initializer(stddev=0.1)\n        biases = tf.zeros_initializer()\n        \n        dense = tf.contrib.layers.fully_connected(outputs[:, -1], num_outputs = fc_units, activation_fn = tf.sigmoid,weights_initializer = weights,biases_initializer = biases)\n        dense = tf.contrib.layers.dropout(dense, keep_prob)\n        \n        # Depending on the iteration, use a second fully connected layer\n        if multiple_fc == True:\n            dense = tf.contrib.layers.fully_connected(dense,\n                        num_outputs = fc_units,\n                        activation_fn = tf.sigmoid,\n                        weights_initializer = weights,\n                        biases_initializer = biases)\n            \n            dense = tf.contrib.layers.dropout(dense, keep_prob)\n    \n    # Make the predictions\n    with tf.name_scope('predictions'):\n        predictions = tf.contrib.layers.fully_connected(dense, \n                          num_outputs = 1, \n                          activation_fn=tf.sigmoid,\n                          weights_initializer = weights,\n                          biases_initializer = biases)\n        \n        tf.summary.histogram('predictions', predictions)\n    \n    # Calculate the cost\n    with tf.name_scope('cost'):\n        cost = tf.losses.mean_squared_error(labels, predictions)\n        tf.summary.scalar('cost', cost)\n    \n    # Train the model\n    with tf.name_scope('train'):    \n        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n\n    # Determine the accuracy\n    with tf.name_scope(\"accuracy\"):\n        correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n        tf.summary.scalar('accuracy', accuracy)\n    \n    # Merge all of the summaries\n    merged = tf.summary.merge_all()    \n\n    # Export the nodes \n    export_nodes = ['inputs', 'labels', 'keep_prob','initial_state', 'final_state','accuracy', 'predictions', 'cost', 'optimizer', 'merged']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n    \n    return graph\n\n\ndef train(model, epochs, log_string):\n    '''Train the RNN'''\n\n    saver = tf.train.Saver()\n    \n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n\n        # Used to determine when to stop the training early\n        valid_loss_summary = []\n        \n        # Keep track of which batch iteration is being trained\n        iteration = 0\n\n        print()\n        print(\"Training Model: {}\".format(log_string))\n\n        train_writer = tf.summary.FileWriter('./logs/3/train/{}'.format(log_string), sess.graph)\n        valid_writer = tf.summary.FileWriter('./logs/3/valid/{}'.format(log_string))\n\n        for e in range(epochs):\n            state = sess.run(model.initial_state)\n            \n            # Record progress with each epoch\n            train_loss = []\n            train_acc = []\n            val_acc = []\n            val_loss = []\n\n            with tqdm(total=len(x_train)) as pbar:\n                for _, (x, y) in enumerate(get_batches(x_train, y_train,batch_size), 1):\n                    feed = {model.inputs: x, model.labels: y[:, None], model.keep_prob: dropout, model.initial_state: state}\n                    summary, loss, acc, state, _ = sess.run([model.merged,model.cost, model.accuracy, model.final_state, \n                                                  model.optimizer], \n                                                  feed_dict=feed)                \n                    \n                    # Record the loss and accuracy of each training  batch\n                    \n                    train_loss.append(loss)\n                    train_acc.append(acc)\n                    \n                    # Record the progress of training\n                    train_writer.add_summary(summary, iteration)\n                    \n                    iteration += 1\n                    pbar.update(batch_size)\n            \n            # Average the training loss and accuracy of each epoch\n            avg_train_loss = np.mean(train_loss)\n            avg_train_acc = np.mean(train_acc) \n\n            val_state = sess.run(model.initial_state)\n            with tqdm(total=len(x_valid)) as pbar:\n                for x, y in get_batches(x_valid,y_valid,batch_size):\n                    feed = {model.inputs: x,\n                            model.labels: y[:, None],\n                            model.keep_prob: 1,\n                            model.initial_state: val_state}\n                    summary, batch_loss, batch_acc, val_state = sess.run([model.merged, model.cost, model.accuracy, model.final_state], \n                                           feed_dict=feed)\n                    \n                    # Record the validation loss and accuracy of each epoch\n                 \n                    val_loss.append(batch_loss)\n                    val_acc.append(batch_acc)\n                    pbar.update(batch_size)\n      \n            # Average the validation loss and accuracy of each epoch\n            avg_valid_loss = np.mean(val_loss)    \n            avg_valid_acc = np.mean(val_acc)\n            valid_loss_summary.append(avg_valid_loss)\n            \n            # Record the validation data's progress\n            valid_writer.add_summary(summary, iteration)\n\n            # Print the progress of each epoch\n            print(\"Epoch: {}/{}\".format(e, epochs),\n                  \"Train Loss: {:.3f}\".format(avg_train_loss),\n                  \"Train Acc: {:.3f}\".format(avg_train_acc),\n                  \"Valid Loss: {:.3f}\".format(avg_valid_loss),\n                  \"Valid Acc: {:.3f}\".format(avg_valid_acc))\n\n            # Stop training if the validation loss does not decrease after 3 epochs\n            \n            if avg_valid_loss > min(valid_loss_summary):\n                print(\"No Improvement.\")\n                stop_early += 1\n                if stop_early == 3:\n                    break   \n            \n            # Reset stop_early if the validation loss finds a new low\n            # Save a checkpoint of the model\n            else:\n                print(\"New Record!\")\n                stop_early = 0\n                checkpoint =\"./sentiment_{}.ckpt\".format(log_string)\n                saver.save(sess, checkpoint)\n\n\n\nn_words = len(tokenizer.word_index)\nembed_size = 50\nbatch_size = 250\nlstm_size = 64\nnum_layers = 2\ndropout = 0.5\nlearning_rate = 0.001\nepochs = 5\nmultiple_fc = False\nfc_units = 250\nfor lstm_size in [64]:\n    for multiple_fc in [True]:\n        for fc_units in [128]:\n            log_string = 'ru={},fcl={},fcu={}'.format(lstm_size,multiple_fc,fc_units)\n            model = build_rnn(n_words = n_words, embed_size = embed_size,batch_size = batch_size,lstm_size = lstm_size,num_layers = num_layers,\n                              dropout = dropout,\n                              learning_rate = learning_rate,\n                              multiple_fc = multiple_fc,\n                              fc_units = fc_units)            \n            train(model, epochs, log_string)\n\ndef make_predictions(lstm_size, multiple_fc, fc_units, checkpoint):\n    '''Predict the sentiment of the testing data'''\n    \n    # Record all of the predictions\n    all_preds = []\n\n    model = build_rnn(n_words = n_words, \n                      embed_size = embed_size,\n                      batch_size = batch_size,\n                      lstm_size = lstm_size,\n                      num_layers = num_layers,\n                      dropout = dropout,\n                      learning_rate = learning_rate,\n                      multiple_fc = multiple_fc,\n                      fc_units = fc_units) \n    \n    with tf.Session() as sess:\n        saver = tf.train.Saver()\n        # Load the model\n        saver.restore(sess, checkpoint)\n        test_state = sess.run(model.initial_state)\n        for _, x in enumerate(get_test_batches(x_valid, \n                                               batch_size), 1):\n            feed = {model.inputs: x,\n                    model.keep_prob: 1,\n                    model.initial_state: test_state}\n            predictions = sess.run(model.predictions,feed_dict=feed)\n            for pred in predictions:\n                all_preds.append(float(pred))\n                \n    return all_preds\npred = make_predictions(64,True,128,\"sentiment_ru=64,fcl=True,fcu=128.ckpt\")\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c274faa038492e2170802a0ca2de5b13b71053e3"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}