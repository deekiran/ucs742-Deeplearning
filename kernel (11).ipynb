{
  "cells": [
    {
      "metadata": {
        "_uuid": "264577b5a8ae170a1d0576c6a5cd89fbbd1f1954"
      },
      "cell_type": "markdown",
      "source": "# Handwritten digits classification using CNN in tensorflow\nThe MNIST dataset comprises 60,000 training examples and 10,000 test examples of the handwritten digits 0â€“9, formatted as 28x28-pixel monochrome images.\n\n"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport tensorflow as tf\n\ntf.logging.set_verbosity(tf.logging.INFO)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "730143bb91c12c8d86dd9f23da385b0dc1d58f9f"
      },
      "cell_type": "markdown",
      "source": "\n  # Model function for CNN\n **  Convolutional layers <br>\n   Pooling layers <br>\n    Dense (fully connected) layers <br>** <br>\n   Input Layer <br>\n   Reshape X to 4-D tensor: [batch_size, width, height, channels]<br>\n   MNIST images are 28x28 pixels, and have one color channel <br>\n   Convolutional Layer #1<br>\n   Computes 32 features using a 5x5 filter with ReLU activation.<br>\n   Padding is added to preserve width and height.<br>\n   Input Tensor Shape: [batch_size, 28, 28, 1]<br>\n   Output Tensor Shape: [batch_size, 28, 28, 32]"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cnn_model_fn(features, labels, mode):\n    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n    conv1 = tf.layers.conv2d(\n        inputs=input_layer,\n        filters=32,\n        kernel_size=[5, 5],\n        padding=\"same\",\n        activation=tf.nn.relu)\n\n  # Pooling Layer #1\n  # First max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 28, 28, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 32]\n    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n\n  # Convolutional Layer #2\n  # Computes 64 features using a 5x5 filter.\n  # Padding is added to preserve width and height.\n  # Input Tensor Shape: [batch_size, 14, 14, 32]\n  # Output Tensor Shape: [batch_size, 14, 14, 64]\n    conv2 = tf.layers.conv2d(\n        inputs=pool1,\n        filters=64,\n        kernel_size=[5, 5],\n        padding=\"same\",\n        activation=tf.nn.relu)\n\n  # Pooling Layer #2\n  # Second max pooling layer with a 2x2 filter and stride of 2\n  # Input Tensor Shape: [batch_size, 14, 14, 64]\n  # Output Tensor Shape: [batch_size, 7, 7, 64]\n    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n\n  # Flatten tensor into a batch of vectors\n  # Input Tensor Shape: [batch_size, 7, 7, 64]\n  # Output Tensor Shape: [batch_size, 7 * 7 * 64]\n    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n\n  # Dense Layer\n  # Densely connected layer with 1024 neurons\n  # Input Tensor Shape: [batch_size, 7 * 7 * 64]\n  # Output Tensor Shape: [batch_size, 1024]\n    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n\n  # Add dropout operation; 0.6 probability that element will be kept\n    dropout = tf.layers.dropout(\n    inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n\n  # Logits layer\n  # Input Tensor Shape: [batch_size, 1024]\n  # Output Tensor Shape: [batch_size, 10]\n    logits = tf.layers.dense(inputs=dropout, units=10)\n\n    predictions = {\n      # Generate predictions (for PREDICT and EVAL mode)\n        \"classes\": tf.argmax(input=logits, axis=1),\n      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n      # `logging_hook`.\n        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n  }\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n\n  # Calculate Loss (for both TRAIN and EVAL modes)\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n\n  # Configure the Training Op (for TRAIN mode)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(\n            loss=loss,\n            global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n  # Add evaluation metrics (for EVAL mode)\n    eval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(\n            labels=labels, predictions=predictions[\"classes\"])}\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e23b6495b9eb31c8c7c0d1fc4c566261e6386df8"
      },
      "cell_type": "code",
      "source": "def main(unused_argv):\n  # Load training and eval data\n    mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n    train_data = mnist.train.images  # Returns np.array\n    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n    eval_data = mnist.test.images  # Returns np.array\n    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n\n      # Create the Estimator\n    mnist_classifier = tf.estimator.Estimator(\n    model_fn=cnn_model_fn, model_dir=\"../input/mnist_convnet_model\")\n\n  # Set up logging for predictions\n  # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)\n\n  # Train the model\n    train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n       x={\"x\": train_data},\n       y=train_labels,\n       batch_size=100,\n       num_epochs=None,\n       shuffle=True)\n    mnist_classifier.train(\n       input_fn=train_input_fn,\n       steps=20000,\n       hooks=[logging_hook])\n\n  # Evaluate the model and print results\n    eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(\n        x={\"x\": eval_data}, y=eval_labels, num_epochs=1, shuffle=False)\n    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n    print(eval_results)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "595df1f5d1cb3ab96a4fe58b88fbb7421792c28c"
      },
      "cell_type": "code",
      "source": "if __name__ == \"__main__\":\n    tf.app.run()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "793286f47adbc377ab25960d6f4037d9aa3f8c58"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}